----- tensorflow를 스터디 하니
      -- 목적
        . 좋던 싫던 쌓여가는 데이터에 생명을 불어넣고 의미를 부여함으로써 시사점을 얻고
        . 그 시사점을 바탕으로 새로운 정책을 만들거나 신규 비즈니스를 개발할 수 있다

      -- 내용
        . 데이터에 대한 특징, 연관성을 찾아서(문과적 관점/인간)
        . 규칙화하고 수식화하는것이 목표인데(이과적 관점/기계)
        . 그렇게 만들어진 수식을 tensorflow에서는 퍼셉트론이라고 함
        . 퍼셉트론은 weight과 bios로 이루어진 수식(y = w1x1 + w2x2 + ... + b)
        . machine learning을 한다는 것은 w와 b를 찾는것이다(x, y는 이미 데이터가 있다)
        . 일반적인 프로그램 관점에서는 y를 찾는것인데 반해 데이터과학 관점에서는 규칙(w와 b)을 찾는것이다
        . w와 b에 아주 작은 적당한 값들을 적용해 가면서(머신러닝) 퍼셉트론식을 완성해 간다
        . 이렇게 만들어진 페셉트론 식을 우리는 model이라고 정의한다
        . 잘 만들어진 model일 수록 의미있는 예측이 가능하다
        . 잘 만들어진 model이란 loss(실제값과 예측값의 오차)가 0에 가깝다는 것을 의미한다

      -- 깨달음
        . model을 어떻게 정의하냐에 따라 의미있는 예측이 가능한데..
        . model을 잘 정의하려면 데이터를 잘 알아야 한다
        . 좋은 model을 만들려면 학습 데이터를 잘 만드는 것이 중요하다
        . 학습 데이터의 대략적인 특성(최대값, 최소값, 평균값, 분포 등)을 파악해야
        . 어떤 shape으로 만들지를 결정할 수 있다
        . 이미 정리된 모델(MNIST, ResNet, EfficientNet)을 적용하려면 그에 맞는 학습데이터를 만들어야 한다(2, 3차원 배열형태 등)
        . 그러기 위해서는
        . 첫째, 기본적으로 모든 데이터를 수치화 해야하고
        . 둘째, 모델 적용을 위한 적당한 데이터 조작이 필요하다(정규화, Flatten, reshape 등등)
        . Python이 이런 작업에 나름 최적화된 프로그램밍 언어이다

----- 인공지능
        |--머신러닝(대표 library:scikit-learn) : 인공지능 중 하나의 분야
            |--선형모델
            |--트리모델
            |--군집알고리즘
            |--딥러닝(대표 library:tensorflow) : 수많은 머신러닝 알고리즘 중 인공신경망을 이용한 알고리즘

----- AI(Artificial Intelligence)
      -- Symbolic AI
         . 데이터 & 규칙 --> 전통적인 프로그래밍 --> 해답
      -- 머신러닝
         . 데이터 & 해답 --> 머신러닝 --> 규칙
         . 샘플과 기댓값이 주어졌을 때 데이터 처리 작업을 위한 실행 규칙을 찾는 것
         . 찾아진 규칙을 적용하여 새로운 데이터에 대한 해답을 예측

----- 머신러닝
      . training을 통해 데이터에서 통계적 구조를 찾아 규칙을 만드는 작업
      . 작업의 성격은 수리통계와 밀접하고 수학적 이론을 기반으로 하지만(너무 어렵쟎아~)
      . 수학적 관점을 추상화하여 엔지니어링 지향적으로 접근.(특히 딥러닝)

----- 딥러닝
      . 데이터로부터 모델을 만드는 데 얼마나 많은 층을 사용했는지가 그 모델의 깊이를 나타내며,
      . '딥'이란 깊은 통찰의 의미가 아니라 그 모델의 연속된 층을 학습한다는 개념(다단계 정보 추출 작업)
      . 머신러닝 접근 방법은 1~2개의 층을 학습
      . 딥러닝 접근 방법은 수십, 수백개의 층을 학습 --> 신경망(Neural Network)
      . but, 뇌와 같은 신경 생물학과는 전혀 상관없음. 단순 수학모델 일뿐
      . 2010년초에 기존 머신러닝으로는 어려웠던 시각과 청각같은 지각의 문제에서 두각을 나타내며 유명해짐

----- 분류
      . Supervised Learning : 학습 데이터에 정답이 있는 정답예측 모델(개/고양이 분류)
      . Unsupervised Learning : 학습 데이터에 정답이 없음(비슷한거 추천 알고리즘 활용)
      . Reinforcement Learning : 게임학습

----- activation function
      . hidden layer에 의미를 부여하여 비선형적인 예측을 적용할때
      . activation='swish'
      . activation function을 적용하지 않으면 hidden layer는 의미가 없어지며, Neural Network가 아님

----- 러닝머쉰이란
      . 예측값과 실제값의 오차를 최소화하는 가중치값 및 bios을 찾는 행위

----- learning rate optimizer
      . loss 최소값을 효율적으로 빨리 찾기위한 도구(정답은 없고 여러개 적용후 선택)
      . 효율적인 학습을 위한 최적화된 알고리즘
      . 일반적으로 adam 알고리즘 적용

----- programming VS. machine learning
      . programming : data + rule(특징, feature, 가중치) ==> answer
      . machine learning : data + answer ==> rule(특징, feature, 가중치)

----- 모델 만드는 방식
      . Sequential API : 순차적 처리(Stack)
      . Functional API : 프로세스 처리(명시적 입출력 정의)
      . Subclassing : ???

----- MNIST 데이터
      . 학습 데이터(mnist.train) : 학습지, 참고서 55,000건
      . 테스트 데이터(mnist.text) : 모의고사      10,000건
      . 검증 데이터(mnist.validation) : 본고사    5,000건

----- 데이터셋
      . xs: 학습 이미지 mnist.train.images
      . ys: 학습 라벨 mnist.train.labels

----- one-hot 벡터
      . 단 하나의 차원에서만 1이고, 나머지 차원에서는 0인 벡터
      . 서로 다른 여러 항목 중 하나일 확률을 계산할때 최적

----- softmax regression
      예측 확률           가중치             입력값  바이어스(bias)
      |y1|              |W11, W12, W13|   |x1|   |b1|
      |y2|     = softmax|W21, W22, W23| * |x2| + |b2|
      |y3|              |W31, W32, W33|   |x3|   |b3|

----- 크로스 엔트로피
      . 우리의 예측이 실제 값을 설명하기에 얼마나 비효율적인지를 측정하는 것

----- 경사 하강법(gradient descent algorithm)
      . 텐서플로우가 각각의 변수 비용을 줄이는 방향으로 조금씩 이동

----- 확률적 학습(stochastic training)
      . 무작위 데이터의 작은 배치를 사용하는 방법(5만개 중 무작위 100개를 선택)

----- 퍼셉트론(Perceptron)
      . Perception(인지능력) + Neuron(신경세포) = 인공뉴런
      . 학습(머신러닝)이란 퍼셉트론의 오류가 최소화될 수 있는 방향으로 가중치를 조금씩 조정해가는 것
      . 가중치(weight)와 편향(Bias)을 고려하여 만든 수식
        y1 = x1w1 + x2w2 + ... + b

----- 데이터 Type
      . Scalar(0D 텐서) : 1
      . Vector(1D 텐서) : [1,2,3]
      . Matrix(2D 텐서) : [[1,2,3],[4,5,6]]
      . Tensor(3D 이상) : [[[1,2,3],[4,5,6],[7,8,9]]]

----- Tensor(다차원 넘파이 배열: 숫자를 위한 컨테이너)
      . 데이터 자체: 차원 = 변수(컬럼)
      . 데이터 형태(shape): 차원 = 배열의 깊이
      . Black & White image : (3, 7), 2차원의 형태, 21차원 이미지 ==> 학습이미지 50000장 (50000, 3, 7)
      . RGB image : (3, 7, 3), 3차원의 형태, 63차원 이미지 ==> 학습이미지 60000장 (60000, 3, 7, 3)

----- Convolution(합성곱)
      . 특정한 패턴의 특징이 어디서 나타나는지를 확인하는 도구
      . 이미지의 특징을 찾는것
      . 8은 위아래 o이 각 1개씩

----- 필터
      . 필터 1개에 1개의 이미지 생성
      . 필터셋은 3차원 형태로 된 가중치의 모음

----- CNN(Convolution Neural Network)
      . Convolution Layer :
      . Max Pooling Layer : 특징적인 가장큰 숫자를 남기면서 이미지 size를 축소
      . Flatten Layer : data를 1차원으로 펼침

----- activation='softmax'
      . 10개중 하나를 선택해야 하는 case에 적용
      . 보통 최종 Dense Layer에서 사용
      . 10개 각각의 확률 중 가장 큰 것을 1로 만들고 나머지는 모두 0으로 만드는 방식

----- 정규화(min-max 스케일링)
      . x-x.min() / x.max()-x.min()
      . 모든값이 0~1 사이에 존재
      . 정규화를 하면 경사하강법(Gradient Descent)으로 최저점 접근이 수월하여 학습이 효과적(수렴속도가 빠름)으로 수행됨

----- 표준화(거리 특성에 민감한 알고리즘에 필수 적용)
      . train_scaled = train_x - mean() / 표준편차
      . test_scaled = test_x - mean() / 표준편차 # test_scaled에서도 train_x의 평균과 표준편차 적용
      . 분포의 분산이 1이 되도록
      . 상한, 하한이 없어 특정 알고리즘에서는 문제가 될 수 있음
      . test_x를 전처리할 경우에도 train_scaled을 기준으로 처리함

----- loss 최소화 찾기 메커니즘
      . 모든 알고리즘은 loss를 최소화 하는게 목표

                      *
                       *   * *
      Local Optimum ==> * *   *     *
                               *   *
             Global Optimum ==> * *

      . SGD(Stockastic Gradient Descent:확률적 경사하강법)은 머쉰/딥러닝 알고리즘이 아니라 대상알고리즘을 적용하여 훈련/최적화 방법이다.
      . 회귀(Regression) 알고리즘은 미분가능(연속된)하여 경사하강법을 적용한 손실함수를 사용(성능지표=손실)
        --> 회귀 알고리즘은 성능지표인 손실을 최소화하기 위해 경사하강법을 적용
      . 분류 알고리즘은 미분가능하지 않아(0혹은 1이므로 불연속) 경사하강법이 아닌 정확도 성능을 사용하므로 로지스틱 손실함수를 사용한다.(성능지표=정확도)
        --> 분류 알고리즘은 성능지표인 정확도를 최대화(손실 최소화)하기 위해 로지스틱 손실함수(binary_crossentropy)를 적용

----- 과소적합(underfitting)
      . 머신러닝 모델이 훈련 데이터보다 새로운 데이터(test데이터 혹은 실데이터)에서 성능이 높아지는 현상
      . neighbors 수를 크게 할수록 훈련데이터와 dependency가 높아지므로 과소적합이 됨

----- 과대적합(overfitting)
      . 머신러닝 모델이 훈련 데이터보다 새로운 데이터(test데이터 혹은 실데이터)에서 성능이 낮아지는 현상
      . 과소적합과 과대적합 사이에 있는 모델이 좋은 모델임

----- 다항회귀 Vs 다중회귀
      . 다항회귀 : 특성 하나(x)를 여러차원으로 사용 y = ax^2 + bx + c
      . 다중회귀 : 특성 여러개(길이, 높이, 두께 등)를 사용(y는 고정)
                 특성 1개 -> 2차원 y = ax + b
                 특성 2개 -> 3차원
                 특성 3개 -> 4차원

----- sklean 변환기, 추정기
      . Transformer: fit(), transform()
      . Estimator: fit(), score(), predict()

----- 분류 알고리즘
      . sigmoid : 이진분류
                  lineare 함수 z = a*무게 + b*길이 + c*대각선 + d*높이 + e*두께 + f
      . softmax : 다중분류(원리는 이진분류를 여러번 적용)
                  특성수 만큼의 z함수가 만들어 지며, 특성수 만큼의 z값들은 sigmoid 함수를 적용하여 확률로 만든다

----- Dense층 정의시 고려사항
      . 유닛(차원)을 늘리면 신경망이 더욱 복잡한 표현을 학습할 수 있지만 계산 비용이 커지고 원하지 않는 패턴을 학습할 수도 있음
        즉, train_data에 대한 성능은 높아지지만 test_data에 대해서는 오히려 성능 저하의 요인이 될 수 있음
      1. 얼마나 많은 층을 설정할 것인가?
      2. 얼마나 많은 유닛(차원)을 설정할 것인가?

----- train, validation, test data
      -- train_test_split 2번 수행해서 총 3등분 |---------train---------|----val----|---test---|
      . train_x (60%)
      . val_x (20%) <-- 파라미터 튜닝용
      . test_x (20%) <-- 실전테스트용

      * Machine Learning에서는 교차검증 수행을 권장.
      * Deep Learning에서는 기본적으로 교차검증을 수행하지 않음.

----- 정형, 비정형 데이터
      . ML : Database, Excel, csv 형식에 최적화
      . DL : 오디오, 이미지, 영상, 비정형 text 형식에 최적화

----- keras 분류 loss알고리즘
      . 이진분류: loss='binary_crossentropy'
      . 다중분류: loss='categorical_crossentropy'

----- 자연어 처리단계
      1) 수집(Acquisition)
      2) 점검 및 탐색(Inspection and exploration) : 변수유형, 데이터 특징 분석(최대,최소,분포 등)
      3) 전처리 및 정제(Preprocessing and Cleaning) : 토큰화, 정규화, 불용어 제거 등
      4) 모델링 및 훈련(Modeling and Training) : train_x, val_x
      5) 평가(Evaluation) : test_x
      6) 배포(Deployment)
